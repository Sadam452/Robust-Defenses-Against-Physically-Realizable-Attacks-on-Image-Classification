# -*- coding: utf-8 -*-
"""cifar10CnnGuassianFINAL-Roa/Doa_Dl_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lqmV-D4lD6iOuUgg2uGvWZrmuXMxhgPx
"""

# The code is based on https://colab.research.google.com/github/omerbsezer/Fast-Pytorch/blob/master/Learning_Pytorch/CNN_Mnist.ipynb#scrollTo=3ve5cf1rBYJ-
import torch
import torch.nn as nn
import torchvision.transforms as transforms
import torchvision.datasets as datasets
from torch.autograd import Variable
import torchvision
torch.cuda.is_available()
import matplotlib.pyplot as plt

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import numpy as np
import os
import time
# to count skip rectangles
skip_ctr = 0

class ROA(object):
    '''
    Make sticker
    '''

    def __init__(self, base_classifier, size):
        self.base_classifier = base_classifier
        self.img_size = size
        """
        :param base_classifier: maps from [batch x channel x height x width] to [batch x num_classes]
        :param size: the image size
        """

    def exhaustive_search(self, X, y, alpha, num_iter, width, height, xskip, yskip,random = False):
        """
        :param X: images from the pytorch dataloaders
        :param y: labels from the pytorch dataloaders
        :param alpha: the learning rate of inside PGD attacks
        :param num_iter: the number of iterations of inside PGD attacks
        :param width: the width of ROA
        :param height: the height of ROA
        :param xskip: the skip (stride) when searching in x axis
        :param yskip: the skip (stride) when searching in y axis
        :param random: the initialization the ROA before inside PGD attacks,
                       True is random initialization, False is 0.5 initialization
        """

        with torch.set_grad_enabled(False):

            model = self.base_classifier
            size = self.img_size

            model.eval()
            device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
            X = X.to(device)
            y = y.to(device)

            max_loss = torch.zeros(y.shape[0]).to(y.device) - 100
            all_loss = torch.zeros(y.shape[0]).to(y.device)

            xtimes = (size-width) //xskip
            ytimes = (size-height)//yskip

            output_j = torch.zeros(y.shape[0])
            output_i = torch.zeros(y.shape[0])

            count = torch.zeros(y.shape[0])
            ones = torch.ones(y.shape[0])

            for i in range(xtimes):
                for j in range(ytimes):
                    region = X[:, :, yskip * j:(yskip * j + height//2), xskip * i:(xskip * i + width//2)]

                    # Calculate the average pixel value in the region
                    avg_pixel_value = torch.mean(region)
                    # if torch.all(X[:, :, yskip * j:(yskip * j + height//2), xskip * i:(xskip * i + width//2)] == 0):
                    #     # Check if the background is completely black, if so, skip further checking
                    #     # print("skipped")
                    #     continue
                    if avg_pixel_value > 0.9:
                      skip_ctr += 1
                      continue
                    sticker = X.clone()
                    sticker[:,:,yskip*j:(yskip*j+height),xskip*i:(xskip*i+width)] = 1/2
                    all_loss = nn.CrossEntropyLoss(reduction='none')(model(sticker),y)
                    padding_j = torch.zeros(y.shape[0]) + j
                    padding_i = torch.zeros(y.shape[0]) + i
                    output_j[all_loss > max_loss] = padding_j[all_loss > max_loss]
                    output_i[all_loss > max_loss] = padding_i[all_loss > max_loss]
                    count +=  (all_loss == max_loss).type(torch.FloatTensor)
                    max_loss = torch.max(max_loss, all_loss)

            same_loss = np.transpose(np.argwhere(count>=xtimes*ytimes*0.9))
            for ind in same_loss:
                output_j[ind] = torch.randint(ytimes,(1,)).float()
                output_i[ind] = torch.randint(xtimes,(1,)).float()

            # zero_loss =  np.transpose(np.argwhere(max_loss.cpu()==0))
            # for ind in zero_loss:
            #     output_j[ind] = torch.randint(ytimes,(1,))
            #     output_i[ind] = torch.randint(xtimes,(1,))
            zero_loss_idx = (max_loss == 0).nonzero()  # Find indices where max_loss is zero
            for ind in zero_loss_idx:
                if len(ind) > 0:
                  output_j[ind[0]] = torch.randint(ytimes, (1,))
                if len(ind) > 1:
                  output_i[ind[1]] = torch.randint(xtimes, (1,))


        with torch.set_grad_enabled(True):
            return self.inside_pgd(X,y,width, height,alpha, num_iter, xskip, yskip, output_j, output_i )



    def gradient_based_search(self, X, y, alpha, num_iter, width, height, xskip, yskip, potential_nums,random = False):
        """
        :param X: images from the pytorch dataloaders
        :param y: labels from the pytorch dataloaders
        :param alpha: the learning rate of inside PGD attacks
        :param num_iter: the number of iterations of inside PGD attacks
        :param width: the width of ROA
        :param height: the height of ROA
        :param xskip: the skip (stride) when searching in x axis
        :param yskip: the skip (stride) when searching in y axis
        :param potential_nums: the number of keeping potential candidate position
        :param random: the initialization the ROA before inside PGD attacks,
                       True is random initialization, False is 0.5 initialization
        """

        model = self.base_classifier
        size = self.img_size

        device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

        # Cache for gradients
        cached_gradients = {}

        gradient = torch.zeros_like(X, requires_grad=True).to(device)
        X1 = torch.zeros_like(X, requires_grad=True).to(device)
        X = X.to(device)
        y = y.to(device)
        X1.data = X.detach().to(device)

        # Check if gradients for this input are already cached
        input_key = (X.shape, y.shape, device)
        if input_key not in cached_gradients:
            loss = nn.CrossEntropyLoss()(model(X1), y)
            loss.backward()
            gradient.data = X1.grad.detach()
            cached_gradients[input_key] = gradient.data
        else:
            gradient.data = cached_gradients[input_key]

        max_val, _ = torch.max(torch.abs(gradient.view(gradient.shape[0], -1)), 1)
        gradient = gradient / max_val[:, None, None, None]
        X1.grad.zero_()

        xtimes = (size - width) // xskip
        ytimes = (size - height) // yskip

        nums = potential_nums
        output_j1 = torch.zeros(y.shape[0]).repeat(nums).view(y.shape[0], nums)
        output_i1 = torch.zeros(y.shape[0]).repeat(nums).view(y.shape[0], nums)
        matrix = torch.zeros([ytimes * xtimes]).repeat(1, y.shape[0]).view(y.shape[0], ytimes * xtimes)
        max_loss = torch.zeros(y.shape[0]).to(y.device)
        all_loss = torch.zeros(y.shape[0]).to(y.device)

        # Cache for average pixel values
        cached_avg_values = {}

        for i in range(xtimes):
            for j in range(ytimes):
                region_key = (i, j)
                if region_key not in cached_avg_values:
                    region = X[:, :, yskip * j:(yskip * j + height // 2), xskip * i:(xskip * i + width // 2)]
                    avg_pixel_value = torch.mean(region)
                    cached_avg_values[region_key] = avg_pixel_value
                else:
                    avg_pixel_value = cached_avg_values[region_key]

                # Skip checks based on average pixel value
                # if avg_pixel_value > 0.9:
                #     continue
                # if torch.all(X[:, :, yskip * j:(yskip * j + height//2), xskip * i:(xskip * i + width//2)] == 0):
                #     # Check if the background is completely black, if so, skip further checking
                #     # print("skipped")
                #     continue
                if avg_pixel_value > 0.9:
                  skip_ctr += 1
                  continue
                num = gradient[:,:,yskip*j:(yskip*j+height),xskip*i:(xskip*i+width)]
                loss = torch.sum(torch.sum(torch.sum(torch.mul(num,num),1),1),1)
                matrix[:,j*xtimes+i] = loss
        topk_values, topk_indices = torch.topk(matrix,nums)
        output_j1 = topk_indices//xtimes
        output_i1 = topk_indices %xtimes

        output_j = torch.zeros(y.shape[0]) + output_j1[:,0].float()
        output_i = torch.zeros(y.shape[0]) + output_i1[:,0].float()

        with torch.set_grad_enabled(False):
            for l in range(output_j1.size(1)):
                sticker = X.clone()
                for m in range(output_j1.size(0)):
                    sticker[m,:,yskip*output_j1[m,l]:(yskip*output_j1[m,l]+height),xskip*output_i1[m,l]:(xskip*output_i1[m,l]+width)] = 1/2
                sticker1 = sticker.detach()
                all_loss = nn.CrossEntropyLoss(reduction='none')(model(sticker1),y)
                padding_j = torch.zeros(y.shape[0]) + output_j1[:,l].float()
                padding_i = torch.zeros(y.shape[0]) + output_i1[:,l].float()
                output_j[all_loss > max_loss] = padding_j[all_loss > max_loss]
                output_i[all_loss > max_loss] = padding_i[all_loss > max_loss]
                max_loss = torch.max(max_loss, all_loss)

        return self.inside_pgd(X,y,width, height,alpha, num_iter, xskip, yskip, output_j, output_i)




    def inside_pgd(self, X, y, width, height, alpha, num_iter, xskip, yskip, out_j, out_i, random = False):
        model = self.base_classifier
        model.eval()
        sticker = torch.zeros(X.shape, requires_grad=False)
        for num,ii in enumerate(out_i):
            j = int(out_j[num].item())
            i = int(ii.item())
            sticker[num,:,yskip*j:(yskip*j+height),xskip*i:(xskip*i+width)] = 1
        sticker = sticker.to(y.device)


        if random == False:
            delta = torch.zeros_like(X, requires_grad=True)+1/2
        else:
            delta = torch.rand_like(X, requires_grad=True).to(y.device)
            delta.data = delta.data * 255


        X1 = torch.rand_like(X, requires_grad=True).to(y.device)
        X1.data = X.detach()*(1-sticker)+((delta.detach())*sticker)

        for t in range(num_iter):
            loss = nn.CrossEntropyLoss()(model(X1), y)
            loss.backward()
            X1.data = (X1.detach() + alpha*X1.grad.detach().sign()*sticker)
            X1.data = (X1.detach() ).clamp(0,1)
            X1.grad.zero_()
        return (X1).detach()

#show the images
def imshow(inp, title=None):
    """Imshow for Tensor."""
    inp = inp.numpy().transpose((1, 2, 0))
    plt.imshow(inp)
    if title is not None:
        plt.title(title)
    plt.pause(0.001)  # pause a bit so that plots are updated

import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import datasets, transforms
from torch.nn.modules.normalization import LayerNorm
from torch.nn.modules.transformer import TransformerEncoderLayer, TransformerEncoder


transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])
train_transform = transforms.Compose([
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.RandomCrop(32, padding=4),
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

# loading data
train_dataset = datasets.CIFAR10(root='./data', train=True, transform=transform, download=True)
test_dataset = datasets.CIFAR10(root='./data', train=False, transform=transform)

# Define the size of the subset (30%)
subset_size = int(0.01 * len(train_dataset))
subset_size_ = int(0.01 * len(test_dataset))

# Create a random subset of the training dataset
train_dataset = torch.utils.data.random_split(train_dataset, [subset_size, len(train_dataset) - subset_size])[0]

# Create a random subset of the test dataset
test_dataset = torch.utils.data.random_split(test_dataset, [subset_size_, len(test_dataset) - subset_size_])[0]

batch_size = 100
epochs = 3
train_load = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)
test_load = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)

print("Number of images in training set: {}".format(len(train_dataset)))
print("Number of images in test set: {}".format(len(test_dataset)))
print("Number of batches in the train loader: {}".format(len(train_load)))
print("Number of batches in the test loader: {}".format(len(test_load)))
# class GaussianNoise(torch.nn.Module) :
#     def __init__(self, sigma, device='cuda'):
#         super().__init__()
#         self.sigma = sigma
#         print ( device )
#         self.noise = torch.tensor(0.)

#     def forward(self,x):
#         if self.training and self.sigma != 0:
#             scale = self.sigma * x.detach()
#             sampled_noise = self.noise.repeat(*x.size()).normal_() * scale
#             x = x + sampled_noise
#         return x

# class ConvNet(nn.Module):
#     def __init__(self, dropout, sigma, enable_transformer=True):
#         super(ConvNet, self).__init__()
#         self.sigma = sigma
#         self.enable_dropout = dropout
#         self.enable_transformer = enable_transformer

#         if self.sigma:
#             self.noise1 = GaussianNoise(self.sigma)
#             self.noise2 = GaussianNoise(self.sigma)

#         if self.enable_dropout:
#             self.dropout1 = nn.Dropout(0.25)
#             self.dropout2 = nn.Dropout(0.5)

#         self.conv1 = nn.Conv2d(in_channels=3, out_channels=128,
#                                kernel_size=3, padding=2)
#         self.conv2 = nn.Conv2d(in_channels=128, out_channels=256,
#                                kernel_size=3, padding=2)
#         self.conv3 = nn.Conv2d(in_channels=256, out_channels=512,
#                                kernel_size=3, padding=2)
#         self.lin1 = nn.Linear(512 * 2 * 2, 1024)
#         self.lin2 = nn.Linear(1024, 10)

#         if self.enable_transformer:
#             self.transformer_encoder_layer = TransformerEncoderLayer(d_model=512, nhead=8)
#             self.transformer_encoder = TransformerEncoder(self.transformer_encoder_layer, num_layers=6)

    # def forward(self, x):
    #     # Convolutional layers
    #     x = self.conv1(x)
    #     x = nn.functional.relu(x)
    #     x = nn.functional.max_pool2d(x, kernel_size=2)

    #     x = self.conv2(x)
    #     x = nn.functional.relu(x)
    #     x = nn.functional.max_pool2d(x, kernel_size=2)

    #     x = self.conv3(x)
    #     x = nn.functional.relu(x)
    #     x = nn.functional.max_pool2d(x, kernel_size=2)

    #     # Flatten
    #     x = x.view(-1, 512 * 2 * 2)

    #     # Linear layers
    #     x = self.lin1(x)
    #     x = nn.functional.relu(x)
    #     x = self.lin2(x)

    #     # Apply Transformer if enabled
    #     if self.enable_transformer:
    #         x = x.unsqueeze(1)  # Add sequence dimension
    #         x = self.transformer_encoder(x)
    #         x = x.squeeze(1)  # Remove sequence dimension

    #     return x

#     def forward(self, x):
#         x = self.conv1(x)
#         if self.sigma:
#             x = self.noise1(x)
#         x = F.relu(x)
#         x = F.max_pool2d(x, 2)
#         x = self.conv2(x)
#         x = F.relu(x)
#         x = F.max_pool2d(x, 2)
#         if self.enable_dropout:
#             x = self.dropout1(x)
#         x = self.conv3(x)
#         x = F.relu(x)
#         x = F.max_pool2d(x, 2)
#         x = F.avg_pool2d(x, 2)
#         x = x.view(-1, 512 * 2 * 2)
#         x = self.lin1(x)
#         x = F.relu(x)
#         if self.enable_dropout:
#             x = self.dropout2(x)
#         if self.sigma:
#             x = self.noise2(x)
#         x = self.lin2(x)
#         output = F.log_softmax(x, dim=1)
#         return output

# CUDA = torch.cuda.is_available()
# model = ConvNet(dropout=True, sigma=0.4)

class GaussianNoise(torch.nn.Module):
    def __init__(self, sigma, device='cuda'):
        super().__init__()
        self.sigma = sigma
        self.noise = torch.tensor(0.)

    def forward(self, x):
        if self.training and self.sigma != 0:
            scale = self.sigma * x.detach()
            sampled_noise = self.noise.repeat(*x.size()).normal_() * scale
            x = x + sampled_noise
        return x

class ConvNet(nn.Module):
    def __init__(self, dropout, sigma, enable_transformer=True):
        super(ConvNet, self).__init__()
        self.sigma = sigma
        self.enable_dropout = dropout
        self.enable_transformer = enable_transformer

        if self.sigma:
            self.noise1 = GaussianNoise(self.sigma)
            self.noise2 = GaussianNoise(self.sigma)

        if self.enable_dropout:
            self.dropout1 = nn.Dropout(0.25)
            self.dropout2 = nn.Dropout(0.5)

        self.conv1 = nn.Conv2d(in_channels=3, out_channels=128,
                               kernel_size=3, padding=2)
        self.conv2 = nn.Conv2d(in_channels=128, out_channels=256,
                               kernel_size=3, padding=2)
        self.conv3 = nn.Conv2d(in_channels=256, out_channels=512,
                               kernel_size=3, padding=2)
        self.lin1 = nn.Linear(2048, 1024)  # Adjusted input dimension
        self.lin2 = nn.Linear(1024, 10)

        # if self.enable_transformer:
        #     self.transformer_encoder_layer = TransformerEncoderLayer(d_model=2048, nhead=8)  # Adjusted input dimension
        #     self.transformer_encoder = TransformerEncoder(self.transformer_encoder_layer, num_layers=6)

    def forward(self, x):
        x = self.conv1(x)
        if self.sigma:
            x = self.noise1(x)
        x = F.relu(x)
        x = F.max_pool2d(x, 2)
        x = self.conv2(x)
        x = F.relu(x)
        x = F.max_pool2d(x, 2)
        if self.enable_dropout:
            x = self.dropout1(x)
        x = self.conv3(x)
        x = F.relu(x)
        x = F.max_pool2d(x, 2)
        x = F.avg_pool2d(x, 2)
        x = x.view(-1, 2048)  # Adjusted output dimension
        # if self.enable_transformer:
        #     x = x.unsqueeze(0)  # Add batch dimension
        #     x = x.permute(1, 0, 2)  # Reshape for transformer input
        #     x = self.transformer_encoder(x)
        #     x = x.permute(1, 0, 2)  # Reshape back to original
        #     x = x.squeeze(0)  # Remove batch dimension
        x = self.lin1(x)
        x = F.relu(x)
        if self.enable_dropout:
            x = self.dropout2(x)
        if self.sigma:
            x = self.noise2(x)
        x = self.lin2(x)
        output = F.log_softmax(x, dim=1)
        return output

CUDA = torch.cuda.is_available()
model = ConvNet(dropout=True, sigma=0.4)


if CUDA:
    model = model.cuda()
loss_function = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)

iteration=0
res_time = 0
accuracy = 0
accuracy_adv = 0

for epoch in range(epochs):
  print("epoch ",epoch," of ", epochs)
  for i, (images,labels) in enumerate(train_load):
    iteration+=1
    if CUDA:
      images =Variable(images.cuda())
      labels =Variable(labels.cuda())
    else:
      images =Variable(images)
      labels =Variable(labels)
    #initialize the ROA module
    roa = ROA(model, 32)

    learning_rate = 0.1
    iterations = 5
    ROAwidth = 10
    ROAheight = 10
    skip_in_x = 2
    skip_in_y = 2
    potential_nums = 5

##############################################################################
    # Gradient Based Search
    # Image = roa.gradient_based_search(images, labels, learning_rate,\
    #     iterations, ROAwidth , ROAheight, skip_in_x, skip_in_y, potential_nums)
    '''
    press command and click the roa.gradient_based_search to check how to use
    hyperparameter:

    '''

##############################################################################
    #Exhaustive Search
    #print(images.shape)
    start_time = time.time()
    Image = roa.exhaustive_search(images, labels, learning_rate,\
       iterations, ROAwidth , ROAheight, skip_in_x, skip_in_y)
    '''
    press command and click the roa.exhaustive_search to check how to use
    hyperparameter:
    '''
    end_time = time.time()  # End time for measuring execution time
    res_time += end_time - start_time

    # show some images
    # if i == 1:
    #     print(Image.shape)
    #     out = torchvision.utils.make_grid(Image[0:5])
    #     print(out.shape)
    #     imshow(out.cpu())


    optimizer.zero_grad()
    outputs=model(Image)
    loss=loss_function(outputs,labels)
    loss.backward()
    optimizer.step()

    if(i+1)%100 ==0 or (i == len(train_load)-1 and epoch == epochs-1):
        correct =0
        correct_adv = 0
        total =0
        for images,labels in test_load:
            if CUDA:
              images =Variable(images.cuda())
            else:
              images =Variable(images)
            Image = roa.gradient_based_search(images, labels, learning_rate,\
        iterations, ROAwidth , ROAheight, skip_in_x, skip_in_y, potential_nums)
            outputs_adv=model(Image)
            _,predicted_adv=torch.max(outputs_adv.data,1)

            outputs=model(images)
            _,predicted=torch.max(outputs.data,1)

            total+=labels.size(0)
            if CUDA:
              correct += (predicted.cpu()==labels.cpu()).sum()
              correct_adv += (predicted_adv.cpu()==labels.cpu()).sum()
            else:
              correct += (predicted==labels).sum()
              correct_adv += (predicted_adv==labels).sum()

        accuracy = 100 *correct/total
        accuracy_adv = 100 *correct_adv/total
        print("Iteration: {}, Train Loss: {}, Test Accuracy:{}%, Adv_test Accuracy:{}%".format(iteration, loss.item(),accuracy, accuracy_adv))

print("Finished!")
print("Test Accuracy:{}%, Adv_test Accuracy:{}%".format(accuracy, accuracy_adv))
print("Total time take:", res_time, " seconds | Skips :", skip_ctr)